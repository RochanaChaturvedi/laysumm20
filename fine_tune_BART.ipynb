{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Copy of Copy of Untitled.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RochanaChaturvedi/laysumm20/blob/master/fine_tune_BART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIJ4cTvPCK_U"
      },
      "source": [
        "### This notebook is an adaptation of: Text Generation with blurr by Wayde Gilliam\n",
        "https://ohmeow.com/posts/2020/05/23/text-generation-with-blurr.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uHRuifyCK_V"
      },
      "source": [
        "# only run this cell if you are in collab\n",
        "# !pip install ohmeow-blurr\n",
        "!pip install torch==1.6.0 \n",
        "# !pip install nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq1pD_JCCK_Y"
      },
      "source": [
        "import nlp\n",
        "import pandas as pd\n",
        "from fastai.text.all import *\n",
        "from transformers import *\n",
        "\n",
        "from blurr.data.all import *\n",
        "from blurr.modeling.all import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xELtw2ccCK_a"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKrBxRrfvuKz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNIc9YQB83LE"
      },
      "source": [
        "# models=['wMVC_exp2']\n",
        "# # Folder hierarchy:\n",
        "# #summaries\\\\batch3 or summaries\\\\laysumm2 or summaries\\\\test\n",
        "# path=\"/content/drive/My Drive/Laysumm/\"\n",
        "# system_path=path+\"system_summaries/Exp2_clipped_text/ensemble_summaries/batch3/\"\n",
        "# model_path=path+\"gold_summaries/test/\"\n",
        "# # output_path=path+\"system_summaries/Exp2_clipped_text/Evaluation/output/\"\n",
        "# # input_data=path+\"system_summaries/Exp2_clipped_text/ensemble_summaries/laysumm2/mvc/en-wMVC-ClippedExp-L/\"\n",
        "# # input_data=path+\"Preprocessed_data/3_Final_input_data/laysumm2/Laysum2-Abstract-ClippedText/\" #merged summaries\n",
        "# # val_data=path+\"system_summaries/Exp2_clipped_text/ensemble_summaries/batch3/mvc/en-wMVC-ClippedExp-B/\"\n",
        "# test_data=path+\"system_summaries/Exp2_clipped_text/ensemble_summaries/test/mvc/en-wMVC-ClippedExp-test/\"\n",
        "\n",
        "# data={}\n",
        "# i=0\n",
        "# for doc in os.listdir(test_data):\n",
        "#   with open(test_data+doc,'r') as f:\n",
        "#           text = f.read()\n",
        "#           # print(doc,text)\n",
        "#   with open(model_path+doc.replace(\".txt\",\"_LAYSUMM.TXT\"),'r') as f:\n",
        "#           summ=f.read()\n",
        "#           summ=summ.split(\"PARAGRAPH\")[-1]\n",
        "#           # print(summ)\n",
        "    \n",
        "#   data[f]=(text,summ)\n",
        "#   # break\n",
        "# # print(data)\n",
        "# df_test=pd.DataFrame.from_dict(data, orient='index',columns=[\"article\",\"highlights\"])\n",
        "# # df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvCHXy010ssX"
      },
      "source": [
        "df=pd.read_csv(\"/content/drive/My Drive/laysumm_2/laysumm.csv\")\n",
        "df_val=pd.read_csv(\"/content/drive/My Drive/laysumm_2/laysumm-v.csv\")\n",
        "df_test=pd.read_csv(\"/content/drive/My Drive/laysumm_2/laysumm-t.csv\")\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IdnpRDNKb85"
      },
      "source": [
        "df=pd.concat([df,df_val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB6-PtQy9ikf"
      },
      "source": [
        "We begin by getting our hugginface objects needed for this task (e.g., the architecture, tokenizer, config, and model).  We'll use blurr's `get_hf_objects` helper method here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOPDC8zo9QhS"
      },
      "source": [
        "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, model_cls=BartForConditionalGeneration)\n",
        "# hf_model.to('cuda')\n",
        "hf_arch, type(hf_tokenizer), type(hf_config), type(hf_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HjtQ7Y0-DpN"
      },
      "source": [
        "Next we need to build out our DataBlock.  Remember tha a DataBlock is a blueprint describing how to move your raw data into something modelable.  That blueprint is executed when we pass it a data source, which in our case, will be the DataFrame we created above. We'll use a random subset to get things moving along a bit faster for the demo as well.\n",
        "\n",
        "Notice we're specifying `trg_max_length` to constrain our decoder inputs to 250 so that our input/predicted summaries will be padded to 250 rather than the default which is whatever you are using for your encoder inputs (e.g., the text you want summarized)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWk1XPms9QdO"
      },
      "source": [
        "hf_batch_tfm = HF_SummarizationBatchTransform(hf_arch, hf_tokenizer)\n",
        "\n",
        "blocks = ( \n",
        "    HF_TextBlock(hf_arch, hf_tokenizer), \n",
        "    HF_TextBlock(hf_arch, hf_tokenizer, hf_batch_tfm=hf_batch_tfm, max_length=400)\n",
        ")\n",
        "\n",
        "dblock = DataBlock(blocks=blocks, \n",
        "                   get_x=ColReader('article'), \n",
        "                   get_y=ColReader('highlights'), \n",
        "                   splitter=RandomSplitter())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqtapqCn9QZk"
      },
      "source": [
        "dls = dblock.dataloaders(df, bs=1)\n",
        "dls.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMqn2v4w9P4J"
      },
      "source": [
        "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')#check GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmu9FMXY_D4m"
      },
      "source": [
        "It's always a good idea to check out a batch of data and make sure the shapes look right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brzczqCcx-c3"
      },
      "source": [
        "b = dls.one_batch()\n",
        "len(b),b[0]['input_ids'].shape, b[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVjYYbQV_N2z"
      },
      "source": [
        "Even better, we can take advantage of blurr's TypeDispatched version of `show_batch` to look at things a bit more intuitively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34yZKJCX_NJp"
      },
      "source": [
        "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcVHrgPjLvvM"
      },
      "source": [
        "#rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJgWlx0rLxFv"
      },
      "source": [
        "import glob\n",
        "def impose_max_length(summary_text, max_tokens=150):\n",
        "    text = summary_text[0].lower()\n",
        "    text = re.sub(r\"[^a-z0-9]+\", \" \", text)\n",
        "    tokens = re.split(r\"\\s+\", text)\n",
        "    tokens = [x for x in tokens if re.match(r\"^[a-z0-9]+$\", x)]\n",
        "    tokens = tokens[0:min(max_tokens, len(tokens))]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
        "def get_rouge(dframe):\n",
        "            scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
        "            # results = {\"rouge1_f\":[], \"rouge1_r\":[], \"rouge2_f\":[], \"rouge2_r\":[], \"rougeL_f\":[], \"rougeL_r\":[]}\n",
        "\n",
        "            print(\"open ground truth file\")\n",
        "            results={}\n",
        "            default_score = 0.0\n",
        "            index=0\n",
        "            for metric in metrics:#rouge1_f\n",
        "                        dframe[metric+\"_f\"]=0\n",
        "                        dframe[metric + \"_r\"]=0\n",
        "            for index,row in dframe.iterrows():\n",
        "                try:\n",
        "                    reference_summary, submitted_summary = row['highlights'],row['system']\n",
        "                    submitted_summary=impose_max_length(submitted_summary)\n",
        "                    scores = scorer.score(reference_summary.strip(),submitted_summary.strip())\n",
        "                    for metric in metrics:\n",
        "                        dframe.loc[index,metric+\"_f\"]=scores[metric].fmeasure\n",
        "                        # print(row[metric+\"_f\"])\n",
        "                        dframe.loc[index,metric + \"_r\"]=scores[metric].recall\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "                    # print(\"Error for Paper ID %d\", paper_id)\n",
        "                    # for metric in metrics:\n",
        "                    #     results[metric+\"_f\"].append(default_score)\n",
        "                    #     results[metric + \"_r\"].append(default_score)\n",
        "                    \n",
        "\n",
        "            print(\"evaluation finished\")\n",
        "            return dframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgH4EGhpG4tk"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys5tnLAy_Za4"
      },
      "source": [
        "We'll prepare our BART model for training by wrapping it in blurr's `HF_TextGenerationModelWrapper` model object.  This class will handle ensuring all our inputs get translated into the proper arguments needed by a huggingface conditional generation model.  We'll also use a custom model splitter that will allow us to apply discriminative learning rates over the various layers in our huggingface model.\n",
        "\n",
        "Once we have everything in place, we'll freeze our model so that only the last layer group's parameters of trainable.  See [here](https://docs.fast.ai/basic_train.html#Discriminative-layer-training) for our discriminitative learning rates work in fastai.\n",
        "\n",
        "**Note:** This has been tested with BART only thus far (if you try any other conditional generation transformer models they may or may not work ... if you do, lmk either way)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl2NW4CZyyo0"
      },
      "source": [
        "text_gen_kwargs = { **hf_config.task_specific_params['summarization'], **{'max_length': 220, 'min_length': 90, 'length_penalty': 1.5, 'no_repeat_ngram_size': 3} }\n",
        "text_gen_kwargs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GVhlRA8_NGs"
      },
      "source": [
        "model = HF_BaseModelWrapper(hf_model)\n",
        "model_cb = HF_SummarizationModelCallback(text_gen_kwargs=text_gen_kwargs)\n",
        "\n",
        "learn = Learner(dls, \n",
        "                model,\n",
        "                opt_func=ranger,\n",
        "                loss_func=HF_MaskedLMLoss(),\n",
        "                cbs=[model_cb],\n",
        "                splitter=partial(summarization_splitter, arch=hf_arch))#.to_fp16()\n",
        "\n",
        "learn.create_opt() \n",
        "learn.freeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_xhrbcgAtS0"
      },
      "source": [
        "It's also not a bad idea to run a batch through your model and make sure the shape of what goes in, and comes out, looks right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYTk9W-s_NC_"
      },
      "source": [
        "b = dls.one_batch()\n",
        "preds = learn.model(b[0])\n",
        "\n",
        "len(b),b[0]['input_ids'].shape, b[1].shape, len(preds), preds[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p38hho9mz8H4"
      },
      "source": [
        "# print(len(learn.opt.param_groups))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcpCHm2_A68D"
      },
      "source": [
        "Still experimenting with how to use fastai's learning rate finder for these kinds of models.  If you all have any suggestions or interesting insights to share, please let me know.  We're only going to train the frozen model for one epoch for this demo, but feel free to progressively unfreeze the model and train the other layers to see if you can best my results below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYMzd76Z_M_3"
      },
      "source": [
        "# learn.lr_find(suggestions=True)#oom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nLRDgWB_M6A"
      },
      "source": [
        "learn.fit_one_cycle(3, lr_max=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EMy0togEYh0"
      },
      "source": [
        "learn.show_results(learner=learn, max_n=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcmpeMrOBaox"
      },
      "source": [
        "Even better though, blurr augments the fastai Learner with a `generate_text` method that allows you to use huggingface's `PreTrainedModel.generate` method to create something more human-like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwUkWbYJ_M3B"
      },
      "source": [
        "df_val['system']=\"\"\n",
        "for index,row in df_val.iterrows():\n",
        "  row['system'] = learn.generate_text(row['article'], early_stopping=True, num_beams=4, num_return_sequences=1, max_length=150, min_length=60)\n",
        "\n",
        "df_val=get_rouge(df_val)\n",
        "df_val.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0ZjbIfbT9ee"
      },
      "source": [
        "df_val.to_csv(\"/content/drive/My Drive/laysumm-v.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0t4LSCSUHGl"
      },
      "source": [
        "df_test['system']=\"\"\n",
        "for index,row in df_test.iterrows():\n",
        "  row['system'] = learn.generate_text(row['article'], early_stopping=True, num_beams=4, num_return_sequences=1, max_length=150, min_length=60)[0]\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEKc9ky0a0sD"
      },
      "source": [
        "df_test.to_csv(\"/content/drive/My Drive/laysumm-t.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}